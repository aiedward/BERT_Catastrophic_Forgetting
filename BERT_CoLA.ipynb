{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT CoLA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNL4KAHhqV/j6ZcSifZG6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingYannn/BERT_Catastrophic_Forgetting/blob/master/BERT_CoLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4C37AZxzX2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' --test_labels=True\n",
        "!pwd\n",
        "!ls\n",
        "!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n",
        "!ls glue_data/MRPC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXz2zE82zaLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcc94c52-2a02-4ce0-ebbb-62b94d9d488f"
      },
      "source": [
        "GLUE_DIR=\"glue_data/\"\n",
        "mrpc_output = \"mrpc_output\"\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --task_name MRPC \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir $GLUE_DIR/MRPC/ \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_gpu_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 8 \\\n",
        "  --output_dir mrpc_output \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/07/2020 12:45:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/07/2020 12:45:54 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/07/2020 12:45:54 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/07/2020 12:45:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/07/2020 12:45:55 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/07/2020 12:45:58 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "03/07/2020 12:45:58 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/07/2020 12:46:02 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data//MRPC/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=8.0, output_dir='mrpc_output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=500, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/07/2020 12:46:02 - INFO - __main__ -   Loading features from cached file glue_data//MRPC/cached_train_bert-base-uncased_128_mrpc\n",
            "03/07/2020 12:46:02 - INFO - __main__ -   ***** Running training *****\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Num examples = 3668\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Num Epochs = 8\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/07/2020 12:46:02 - INFO - __main__ -     Total optimization steps = 920\n",
            "Epoch:   0% 0/8 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:24,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:22,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:20,  1.40it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:18,  1.41it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:17,  1.42it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:16,  1.42it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:04<01:15,  1.42it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:15,  1.43it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:14,  1.42it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:13,  1.43it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:07<01:13,  1.42it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:12,  1.42it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:09<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:10,  1.41it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:10,  1.41it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:11<01:09,  1.40it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:12<01:09,  1.40it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:08,  1.40it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:08,  1.40it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:14<01:07,  1.39it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:15<01:06,  1.39it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:06,  1.39it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:05,  1.38it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:17<01:04,  1.39it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:18<01:04,  1.38it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.37it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:19<01:03,  1.37it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:20<01:02,  1.37it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:21<01:02,  1.37it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:23<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:24<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:25<00:58,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:26<00:58,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:27<00:57,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:28<00:56,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:29<00:55,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:54,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:54,  1.33it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:31<00:53,  1.32it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:32<00:52,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:52,  1.32it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:51,  1.32it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:34<00:51,  1.31it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:35<00:50,  1.31it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:49,  1.31it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:49,  1.30it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:48,  1.30it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:38<00:47,  1.30it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:47,  1.30it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:46,  1.30it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:45,  1.30it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:44,  1.30it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:43,  1.30it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:43,  1.30it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:42,  1.29it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:41,  1.29it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:41,  1.29it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:40,  1.29it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:39,  1.29it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:48<00:38,  1.30it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:37,  1.30it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:36,  1.30it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:35,  1.31it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:51<00:35,  1.31it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:34,  1.31it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:33,  1.31it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:32,  1.32it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:54<00:31,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.32it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:30,  1.33it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:29,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:57<00:28,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.33it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.33it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:59<00:26,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [01:00<00:25,  1.34it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.34it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:02<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:03<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:06<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.36it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.36it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:08<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:11<00:13,  1.37it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.37it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.37it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.37it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:14<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.37it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:16<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.37it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.37it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:19<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.37it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:22<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.38it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.38it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.53it/s]\u001b[A\n",
            "Epoch:  12% 1/8 [01:24<09:53, 84.76s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:22,  1.38it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:21,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:21,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:20,  1.38it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:19,  1.38it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:19,  1.38it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:18,  1.38it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:17,  1.38it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:16,  1.38it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:16,  1.38it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:07<01:15,  1.38it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:14,  1.38it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.38it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:11,  1.37it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:09,  1.38it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:09,  1.37it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:08,  1.37it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:06,  1.37it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:05,  1.37it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:18<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:21<01:02,  1.36it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:24<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:27<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:52,  1.34it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:51,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:50,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:49,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:48,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.33it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:47,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.33it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:44,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:43,  1.34it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.34it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:41,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:40,  1.34it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:38,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:37,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:34,  1.34it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.34it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.35it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.35it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.35it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.35it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.35it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.35it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.51it/s]\u001b[A\n",
            "Epoch:  25% 2/8 [02:49<08:28, 84.75s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:24,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:21,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:20,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:19,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:19,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:18,  1.36it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:17,  1.36it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:16,  1.36it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:15,  1.36it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.36it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:14,  1.36it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:11<01:13,  1.36it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.36it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:12,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.36it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:14<01:10,  1.36it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.36it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.36it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.36it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.36it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.36it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:56,  1.36it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.35it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:48,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:45,  1.36it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:43,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:42<00:42,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.36it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:39,  1.36it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.35it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.36it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.36it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.36it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:28,  1.36it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:27,  1.36it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.35it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:14,  1.36it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.36it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:18<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.36it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:21<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.35it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.35it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.50it/s]\u001b[A\n",
            "Epoch:  38% 3/8 [04:14<07:03, 84.72s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:24,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:21,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:20,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:19,  1.36it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:19,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:18,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:17,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:17,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:15,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:11<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:13,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:12,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:14<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:17<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:05,  1.35it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.35it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<01:00,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:31<00:54,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:34<00:51,  1.35it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.35it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:49,  1.34it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:37<00:48,  1.34it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:40<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:43,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:42<00:43,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.34it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:45<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.34it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:48<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:51<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:54<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:57<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [01:00<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:03<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.35it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:11<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.36it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:14<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.35it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:17<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:20<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.35it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.35it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.35it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.50it/s]\u001b[A\n",
            "Epoch:  50% 4/8 [05:39<05:39, 84.79s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:24,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:21,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:20,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:20,  1.35it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:19,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:18,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:17,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:15,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:11<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:13,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:12,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:14<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:17<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.35it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.35it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.36it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:56,  1.36it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.35it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 9.130434782608697e-06, \"loss\": 0.3483793140724301, \"step\": 500}\n",
            "03/07/2020 12:52:11 - INFO - transformers.configuration_utils -   Configuration saved in mrpc_output/checkpoint-500/config.json\n",
            "03/07/2020 12:52:12 - INFO - transformers.modeling_utils -   Model weights saved in mrpc_output/checkpoint-500/pytorch_model.bin\n",
            "03/07/2020 12:52:12 - INFO - __main__ -   Saving model checkpoint to mrpc_output/checkpoint-500\n",
            "03/07/2020 12:52:16 - INFO - __main__ -   Saving optimizer and scheduler states to mrpc_output/checkpoint-500\n",
            "\n",
            "Iteration:  35% 40/115 [00:34<02:39,  2.13s/it]\u001b[A\n",
            "Iteration:  36% 41/115 [00:34<02:07,  1.72s/it]\u001b[A\n",
            "Iteration:  37% 42/115 [00:35<01:43,  1.42s/it]\u001b[A\n",
            "Iteration:  37% 43/115 [00:36<01:27,  1.21s/it]\u001b[A\n",
            "Iteration:  38% 44/115 [00:37<01:15,  1.06s/it]\u001b[A\n",
            "Iteration:  39% 45/115 [00:37<01:07,  1.03it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:38<01:01,  1.12it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:39<00:57,  1.19it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:40<00:54,  1.23it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:40<00:51,  1.27it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:41<00:50,  1.30it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:42<00:48,  1.32it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:42<00:47,  1.33it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:43<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:44<00:45,  1.34it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:45<00:44,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:45<00:43,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:46<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:47<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:48<00:41,  1.34it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:48<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:49<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:50<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:51<00:38,  1.34it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:51<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:52<00:37,  1.34it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:53<00:36,  1.34it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:54<00:35,  1.34it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:54<00:35,  1.33it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:55<00:34,  1.33it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:56<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:57<00:33,  1.32it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:57<00:32,  1.32it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:58<00:31,  1.32it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:59<00:31,  1.31it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [01:00<00:30,  1.31it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [01:00<00:29,  1.31it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [01:01<00:28,  1.31it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [01:02<00:28,  1.31it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [01:03<00:27,  1.31it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [01:04<00:26,  1.31it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [01:04<00:25,  1.31it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:05<00:25,  1.31it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:06<00:24,  1.32it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:07<00:23,  1.32it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:07<00:22,  1.33it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:08<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:09<00:21,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:10<00:20,  1.33it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:10<00:19,  1.33it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:11<00:18,  1.34it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:12<00:17,  1.34it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:13<00:17,  1.34it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:13<00:16,  1.34it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:14<00:15,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:15<00:14,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:15<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:16<00:13,  1.35it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:17<00:12,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:18<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:18<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:19<00:10,  1.35it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:20<00:09,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:21<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:21<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:22<00:07,  1.36it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:23<00:06,  1.36it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:24<00:05,  1.36it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:24<00:05,  1.36it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:25<00:04,  1.36it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:26<00:03,  1.36it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:27<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:27<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:28<00:01,  1.36it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:29<00:00,  1.36it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:29<00:00,  1.51it/s]\u001b[A\n",
            "Epoch:  62% 5/8 [07:08<04:18, 86.28s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:23,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:21,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:20,  1.37it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:19,  1.37it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:17,  1.37it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:16,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.37it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:11,  1.37it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:09,  1.37it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:08,  1.37it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.37it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:06,  1.37it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:21<01:02,  1.36it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:24<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.36it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:27<00:56,  1.36it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:32<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:35<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.36it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:38<00:45,  1.36it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:45,  1.36it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:43,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.35it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:49<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:52<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:55<00:29,  1.34it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:28,  1.34it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.34it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.34it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:58<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:23,  1.34it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.35it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.34it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:14,  1.34it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.34it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:11,  1.34it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.35it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:15<00:08,  1.34it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.34it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:18<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.35it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:21<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.35it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.35it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.51it/s]\u001b[A\n",
            "Epoch:  75% 6/8 [08:33<02:51, 85.79s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:24,  1.35it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:22,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:21,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:20,  1.36it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:19,  1.36it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:18,  1.36it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:18,  1.36it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:17,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:15,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:15,  1.34it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:11<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:13,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:12,  1.35it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:14<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.36it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.36it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.36it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:02,  1.36it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.36it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:56,  1.36it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.36it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:48,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:47,  1.36it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.36it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.36it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:45,  1.36it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:44,  1.36it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.36it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:43,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:42<00:42,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.36it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.36it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:39,  1.36it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.36it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.36it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:36,  1.36it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.36it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:34,  1.36it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:33,  1.36it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.36it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.36it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:31,  1.36it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.36it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.36it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.36it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:28,  1.36it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:27,  1.36it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.36it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.36it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:58<00:25,  1.36it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:25,  1.36it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.36it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.36it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:22,  1.36it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:22,  1.36it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.36it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.36it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.37it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.37it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.36it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.36it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.36it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:12<00:11,  1.36it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:11,  1.36it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.36it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:15<00:08,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.36it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.36it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.36it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:18<00:05,  1.36it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.36it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.36it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.36it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:21<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.35it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:23<00:00,  1.36it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.51it/s]\u001b[A\n",
            "Epoch:  88% 7/8 [09:57<01:25, 85.39s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:23,  1.36it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:22,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:21,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:20,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:20,  1.36it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:19,  1.36it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:18,  1.36it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:18,  1.36it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:17,  1.36it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:16,  1.35it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:15,  1.35it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:14,  1.35it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:11<01:13,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:13,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:12,  1.36it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:14<01:11,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:05,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.36it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:02,  1.36it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.36it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:56,  1.36it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.35it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:50,  1.35it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:44,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:43,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:42<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:42,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:41,  1.35it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:45<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:38,  1.35it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:48<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:35,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:53<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:30,  1.36it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:29,  1.36it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:56<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:28,  1.36it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:58<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:01<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:03<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:04<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:05<00:19,  1.36it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:06<00:18,  1.35it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:17,  1.35it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:16,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:09<00:15,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:11<00:13,  1.35it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:12<00:12,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:14<00:10,  1.36it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:15<00:09,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:16<00:08,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:17<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:18<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:20<00:04,  1.36it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:21<00:03,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:22<00:02,  1.35it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:22<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:23<00:01,  1.36it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:24<00:00,  1.36it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:24<00:00,  1.51it/s]\u001b[A\n",
            "Epoch: 100% 8/8 [11:22<00:00, 85.19s/it]\n",
            "03/07/2020 12:57:25 - INFO - __main__ -    global_step = 920, average loss = 0.21014756948165797\n",
            "03/07/2020 12:57:25 - INFO - __main__ -   Saving model checkpoint to mrpc_output\n",
            "03/07/2020 12:57:25 - INFO - transformers.configuration_utils -   Configuration saved in mrpc_output/config.json\n",
            "03/07/2020 12:57:26 - INFO - transformers.modeling_utils -   Model weights saved in mrpc_output/pytorch_model.bin\n",
            "03/07/2020 12:57:26 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/07/2020 12:57:26 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/07/2020 12:57:26 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/07/2020 12:57:29 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/07/2020 12:57:29 - INFO - __main__ -   Evaluate the following checkpoints: ['mrpc_output']\n",
            "03/07/2020 12:57:29 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/07/2020 12:57:29 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/07/2020 12:57:29 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/07/2020 12:57:32 - INFO - __main__ -   Loading features from cached file glue_data//MRPC/cached_dev_bert-base-uncased_128_mrpc\n",
            "03/07/2020 12:57:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/07/2020 12:57:32 - INFO - __main__ -     Num examples = 408\n",
            "03/07/2020 12:57:32 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 51/51 [00:03<00:00, 14.15it/s]\n",
            "03/07/2020 12:57:36 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/07/2020 12:57:36 - INFO - __main__ -     acc = 0.8406862745098039\n",
            "03/07/2020 12:57:36 - INFO - __main__ -     acc_and_f1 = 0.8647875816993464\n",
            "03/07/2020 12:57:36 - INFO - __main__ -     f1 = 0.888888888888889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLt8oJgeiuNU",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Foy1BMnPi1lV",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Using Colab GPU for Training\n",
        "\n",
        "->->->GPU\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y2gM--OAszS",
        "colab_type": "code",
        "outputId": "414f5072-5d4e-4c70-e5d3-f12ec003f146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device names.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNUG99KGilWG",
        "colab_type": "text"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. In the training loop, we will load data onto the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsbX-zyqCCDv",
        "colab_type": "code",
        "outputId": "5c8e1448-7d26-4e30-ec90-68f2cd4a940a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  # Tell PyTorch to use the GPU.\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "  print('No GPU available, using the CPU instead.')\n",
        "  device = torch.device(\"cpu\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_AaHb_tDKjd",
        "colab_type": "code",
        "outputId": "d4a49c5f-bfab-404e-946e-21b017b60a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc0CSFLjlvWJ",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading CoLA Dataset\n",
        "\n",
        "Single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect.\n",
        "\n",
        "We'll use the wget package to download the dataset to the Colab instances' file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLnawTkrEQci",
        "colab_type": "code",
        "outputId": "9d5bcf0a-7e03-4336-d5d2-8e702b604a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdUXfpvmoOhR",
        "colab_type": "text"
      },
      "source": [
        "The dataset is hosted on Github in this repo: https://nyu-mll.github.io/CoLA/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixeIAzm-FFAG",
        "colab_type": "code",
        "outputId": "0d23fc15-4728-4f95-ce73-c65a21fb6048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "  wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOHagJxCoVMx",
        "colab_type": "text"
      },
      "source": [
        "Unzip the dataset to the file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daw010xpIe2l",
        "colab_type": "code",
        "outputId": "ddb06116-2953-4dd0-a56e-ef6208f05ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public'):\n",
        "  !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPoXicU4ooG0",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Parse\n",
        "\n",
        "Both tokenized and raw versions of the data are available.\n",
        "\n",
        "We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we must use the tokenizer provided by the model.\n",
        "\n",
        "We'll use pandas to parse the \"in-domain\" training set and look at a few of its properties and data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMVn0KQXK2qB",
        "colab_type": "code",
        "outputId": "009d2bc7-9d4e-4d69-a418-b6a515835e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Angela characterized Shelly as a lifesaver.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5048</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They're not finding it a stress being in the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Paul exhaled on Mary.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I ordered if John drink his beer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Press the stamp against the pad completely.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3542</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They can very.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6915</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This arch is supporting the weight of the tower.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>That new handle detaches easily.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5857</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Brazilians pumped the oil across the river.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It is a wooden desk.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2389            l-93  ...        Angela characterized Shelly as a lifesaver.\n",
              "5048            ks08  ...  They're not finding it a stress being in the s...\n",
              "3133            l-93  ...                              Paul exhaled on Mary.\n",
              "5955            c_13  ...                  I ordered if John drink his beer.\n",
              "625             bc01  ...        Press the stamp against the pad completely.\n",
              "3542            ks08  ...                                     They can very.\n",
              "6915            m_02  ...   This arch is supporting the weight of the tower.\n",
              "2908            l-93  ...                   That new handle detaches easily.\n",
              "5857            c_13  ...    The Brazilians pumped the oil across the river.\n",
              "4191            ks08  ...                               It is a wooden desk.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FuSn3CvqD4Q",
        "colab_type": "text"
      },
      "source": [
        "The two properties we actually care about are the sentence and its label, which is referred to as the \"acceptibility judgement\" (0=unaccaptable, 1=acceptable).\n",
        "\n",
        "Here are five sentences which are labeled as not grammatically acceptible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdGx0OqMWLK",
        "colab_type": "code",
        "outputId": "d000efd2-ecf2-4dc4-8e1b-5048201935e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6770</th>\n",
              "      <td>We realised that Dr Jones died because he ate ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>Here's a pole for you to kiss the girl who tie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>Jennifer baked at the potatoes.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4651</th>\n",
              "      <td>Kim is resembled by the model in nearly every ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2672</th>\n",
              "      <td>The book sent to Peter.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "6770  We realised that Dr Jones died because he ate ...      0\n",
              "1652  Here's a pole for you to kiss the girl who tie...      0\n",
              "3258                    Jennifer baked at the potatoes.      0\n",
              "4651  Kim is resembled by the model in nearly every ...      0\n",
              "2672                            The book sent to Peter.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgP-mtoBqleE",
        "colab_type": "text"
      },
      "source": [
        "Extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSpvAMFQSbNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1zoOE95sSVD",
        "colab_type": "text"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "In this section, we'll transform our dataset into the format that BERT can be trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z14VJBILseS6",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 BERT Tokenizer\n",
        "\n",
        "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT. The below cell will download this for us. We'll be using the \"uncased\" version here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfsvtXVNYmhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from transformers import BertTokenizer\n",
        "\n",
        "# # Load the BERT tokenizer.\n",
        "# print('Loading BERT tokenizer...')\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYbSdW78zR6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6135f34a-1885-469c-d5bb-14ba3ec96b0b"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(mrpc_output)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfTaapNztSwt",
        "colab_type": "text"
      },
      "source": [
        "Let's apply the tokenizer to one sentence just to see the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtGnuWMPY668",
        "colab_type": "code",
        "outputId": "62f41e12-7412-44d5-86e7-0b530a01e2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print('Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON20NhQwt4KN",
        "colab_type": "text"
      },
      "source": [
        "When we actually convert all of our sentences, we'll use the tokenize.encode function to handle both steps, rather than calling tokenize and convert_tokens_to_ids separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYbumk8ItfbF",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Sentences to IDs\n",
        "\n",
        "The tokenizer.encode function combines multiple steps for us:\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special [CLS] and [SEP] tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "\n",
        "PS: This function can perform truncating for us, but doesn't handle padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAjwRR51bzk3",
        "colab_type": "code",
        "outputId": "868b548b-72da-4c81-c929-3c7e6ce5ee6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXS31VicuPIb",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Padding & Truncating\n",
        "\n",
        "Pad and truncate our sentences so that they all have the same length, MAX_LEN.\n",
        "\n",
        "The maximum sentence length in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgugRDXtchHx",
        "colab_type": "code",
        "outputId": "7a360144-8599-4352-abbd-322b0ac8d5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy19KYHnwReV",
        "colab_type": "text"
      },
      "source": [
        "Given that, let's choose MAX_LEN = 64 and apply the padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF9QJcJAcjil",
        "colab_type": "code",
        "outputId": "a742c627-ebaa-4044-df4f-bd01aa6c48ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TzGFxGAw6a3",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Attention Masks\n",
        "\n",
        "The attention mask simply makes it explicit which tokens are actual words versus which are padding.\n",
        "\n",
        "The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCtiGxv8cl5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Ywavwqxle8",
        "colab_type": "text"
      },
      "source": [
        "## 3.5 Training & Validation Split\n",
        "\n",
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSQ-9x7bcoMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWoJc-Faxvu2",
        "colab_type": "text"
      },
      "source": [
        "## Converting to PyTorch Data Types\n",
        "\n",
        "Our model expects PyTorch tensors rather than numpy.ndarrays, so convert all of our dataset variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9BfLXU_cqWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqhphGydx8vk",
        "colab_type": "text"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaRfCJbecsID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b0YWX_ryPUq",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train Our Classification Model\n",
        "\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8inZRrtayY60",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 BertForSequenceClassification\n",
        "\n",
        "For this task, we first want to modify the pre-trained BERT model to give outptus for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
        "\n",
        "The huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "\n",
        "We'll be using BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "Let's load BERT. There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7tfSsuxcuga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# # linear classification layer on top. \n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "\n",
        "# # Tell pytorch to run this model on the GPU.\n",
        "# model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zObo_bNvBa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2a527b6-1a05-47fe-ccb4-154fa86e5fd7"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "#                     # You can increase this for multi-class tasks.   \n",
        "#     output_attentions = False, # Whether the model returns attentions weights.\n",
        "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "# )\n",
        "\n",
        "#model = BertForSequenceClassification.from_pretrained(mrpc_output)\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model = BertForSequenceClassification.from_pretrained(\"mrpc_output/pytorch_model.bin\", config=config)\n",
        "#tokenizer = BertTokenizer.from_pretrained(mrpc_output)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFM-Jqf0qId",
        "colab_type": "text"
      },
      "source": [
        "We can browse all of the model's parameters by name here.\n",
        "\n",
        "In the below cell, I've printed out the names and dimensions of the weights for:\n",
        "\n",
        "1. The embedding layer.\n",
        "2. The first of the twelve transformers.\n",
        "3. The output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyHMzwgxcxAY",
        "colab_type": "code",
        "outputId": "9ace84e9-3503-4935-e27c-edb71905592b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxgbMVwM-9e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JNkuMUE_HYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6747448-f44d-472b-ba4a-ec430ff70222"
      },
      "source": [
        "if model != model2:\n",
        "  print(\"different\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "different\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr3uD1Qg04qB",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Optimizer & Learning Rate Scheduler\n",
        "\n",
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuing, the author s recommend choosing from the following values:\n",
        "- Batch size: 16, 32 (We chose 32 when creating our DataLoaders).\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5 (We'll use 2e-5).\n",
        "- Number of epochs: 2,3,4 (We'll use 4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZj1A3eudApl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpYTVZXdD4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcrqOqJR1q4z",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Training Loop\n",
        "\n",
        "Below is our training loop. Fundamentally for each pass in our loop we have a training phas and a validation phase. At each pass we need to:\n",
        "\n",
        "Training loop:\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Clear out the gradients calculated in the previous pass. (In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
        "- Forward pass (feed input data through the network)\n",
        "- Backward pass (backpropagation)\n",
        "- Tell the network to update parameters with optimizer.step()\n",
        "- Track variables for monitoring progress\n",
        "\n",
        "Evaluation loop:\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Forward pass (feed input data through the network)\n",
        "- Compute loss on our validation data and track variables for monitoring progress\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmYAeNGn2vFm",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function for calculating accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuKDkBO1dF5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cOAYj_G4drE",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPU92vW6dIKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3HGj-XdJkO",
        "colab_type": "code",
        "outputId": "0f2ec7f3-d380-44a8-e0e3-7a67001586a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:47.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:03.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:33.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:01:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:47.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:33.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:01:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:47.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:33.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:01:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:31.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:47.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:02.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:18.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:33.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:01:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGgNsALy4lYP",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl7uw8pKdMZT",
        "colab_type": "code",
        "outputId": "5fa63512-5e84-49e9-e132-e1eaabcfb0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD7BHVSFV/4WpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQmJeE24rf4",
        "colab_type": "text"
      },
      "source": [
        "# 5. Performance On Test Set\n",
        "\n",
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using Matthew's correlation coefficient because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BShH6rE86h0U",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Data Preparation\n",
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdtuChaqejP7",
        "colab_type": "code",
        "outputId": "568b9bb4-c472-44b3-c674-87cf87853ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vZivNFG6rOS",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Evaluate on Test Set\n",
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8naF99hem-P",
        "colab_type": "code",
        "outputId": "717d8fb4-38b8-45ec-b0b3-3198c3da39c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryagJ1ed60Q0",
        "colab_type": "text"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the Matthews correlation coefficient (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBJXqXPsepbk",
        "colab_type": "code",
        "outputId": "4ea6b198-cc66-4858-9289-95988125e9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTEnsCCkesKO",
        "colab_type": "code",
        "outputId": "5c488803-4f7a-4c46-e910-bf5a09539d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVm65ueT6_Ut",
        "colab_type": "text"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches.\n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdpTkAKqeu7A",
        "colab_type": "code",
        "outputId": "d3e3d5da-70de-4f82-82b5-77219c985767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.3316863613133475,\n",
              " 0.34151450937027694,\n",
              " 0.32328707534629597,\n",
              " 0.7410010097502685,\n",
              " 0.37777777777777777,\n",
              " 0.0,\n",
              " 0.8320502943378436,\n",
              " 0.7704873741021288,\n",
              " 0.8459051693633014,\n",
              " 0.647150228929434,\n",
              " 0.8150678894028793,\n",
              " 0.647150228929434,\n",
              " 0.4622501635210242,\n",
              " 0.6625413488689132,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xlaobzjeyJx",
        "colab_type": "code",
        "outputId": "7a5ce784-b8a6-408d-c48e-0b8dc25cce5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNpQvRape0R-",
        "colab_type": "code",
        "outputId": "55d4abc2-f86d-4d75-d4c2-70895cc834d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKcv5ofDe3zr",
        "colab_type": "code",
        "outputId": "e7ae1104-eb2d-4e6e-90ee-bc49ed2ed46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427960K\n",
            "-rw-r--r-- 1 root root      2K Mar  1 23:43 config.json\n",
            "-rw-r--r-- 1 root root 427719K Mar  1 23:43 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Mar  1 23:43 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Mar  1 23:43 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Mar  1 23:43 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2zfPiige6sJ",
        "colab_type": "code",
        "outputId": "d31a6ff9-2109-4320-b0f8-2a7474cddd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Mar  1 23:43 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqWhVkTMe8IN",
        "colab_type": "code",
        "outputId": "b7dbf0a4-0f88-4bfe-c311-22106a13cb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDQC3IJ7fAZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/My Drive/BERT/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzblWezrgU7v",
        "colab_type": "code",
        "outputId": "0710bcd4-6715-45dc-81c3-ea2d7c91bfe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwE3ZCcThUwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}