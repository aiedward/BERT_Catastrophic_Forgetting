{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_MRPC_CoLA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXZIvvHItvb2T+ALSJTA+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingYannn/BERT_Catastrophic_Forgetting/blob/master/Bert_MRPC_CoLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfzo3yGz0OHw",
        "colab_type": "code",
        "outputId": "ab34e494-6a6b-4ac4-b456-1485a2fb15f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "# !python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' --test_labels=True\n",
        "!pwd\n",
        "!ls\n",
        "!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n",
        "!python download_glue_data.py --data_dir='glue_data' --tasks='CoLA'\n",
        "!ls glue_data/MRPC\n",
        "!ls glue_data/CoLA"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n",
            "--2020-03-28 20:43:57--  https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8225 (8.0K) [text/plain]\n",
            "Saving to: ‘download_glue_data.py’\n",
            "\n",
            "download_glue_data. 100%[===================>]   8.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-03-28 20:43:58 (194 MB/s) - ‘download_glue_data.py’ saved [8225/8225]\n",
            "\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "dev_ids.tsv  msr_paraphrase_test.txt   test.tsv\n",
            "dev.tsv      msr_paraphrase_train.txt  train.tsv\n",
            "dev.tsv  original  test.tsv  train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H08VqXOcp1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "00f0ae7b-34e4-4ba1-edc1-c9b94540d2b3"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 68.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 62.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=fd8044ab0181aec64466fc146479df7fd9c92a857bc5ebbcda13cd471b6308c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIsQXtbWcwrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e16df66-f773-4a57-eeb0-32ac5faaf82d"
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/run_glue.py\n",
        "\n",
        "GLUE_DIR=\"glue_data/\"\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path 'bert-base-uncased' \\\n",
        "  --task_name CoLA \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir $GLUE_DIR/CoLA/ \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_gpu_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 2 \\\n",
        "  --output_dir 'cola_output' \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-28 20:47:21.998440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "03/28/2020 20:47:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/28/2020 20:47:24 - INFO - filelock -   Lock 139855832364032 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/28/2020 20:47:24 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpqmmd2vh5\n",
            "Downloading: 100% 361/361 [00:00<00:00, 361kB/s]\n",
            "03/28/2020 20:47:25 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/28/2020 20:47:25 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/28/2020 20:47:25 - INFO - filelock -   Lock 139855832364032 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
            "03/28/2020 20:47:25 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "03/28/2020 20:47:25 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:47:26 - INFO - filelock -   Lock 139855346478328 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "03/28/2020 20:47:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp9i6uvsx_\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 433kB/s]\n",
            "03/28/2020 20:47:27 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/28/2020 20:47:27 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/28/2020 20:47:27 - INFO - filelock -   Lock 139855346478328 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "03/28/2020 20:47:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "03/28/2020 20:47:28 - INFO - filelock -   Lock 139855346478776 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/28/2020 20:47:28 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpe2hwcvhm\n",
            "Downloading: 100% 440M/440M [00:29<00:00, 14.7MB/s]\n",
            "03/28/2020 20:47:59 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/28/2020 20:47:59 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/28/2020 20:47:59 - INFO - filelock -   Lock 139855346478776 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "03/28/2020 20:47:59 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "03/28/2020 20:48:01 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "03/28/2020 20:48:01 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/28/2020 20:48:17 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data//CoLA/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, output_dir='cola_output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/28/2020 20:48:17 - INFO - __main__ -   Creating features from dataset file at glue_data//CoLA/\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   Writing example 0/8551\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2028 2062 18404 2236 3989 1998 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2028 2062 18404 2236 3989 2030 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 2062 2057 2817 16025 1010 1996 13675 16103 2121 2027 2131 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   input_ids: 101 2154 2011 2154 1996 8866 2024 2893 14163 8024 3771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:48:17 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:48:18 - INFO - __main__ -   Saving features into cached file glue_data//CoLA/cached_train_bert-base-uncased_128_cola\n",
            "03/28/2020 20:48:20 - INFO - __main__ -   ***** Running training *****\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Num examples = 8551\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Num Epochs = 2\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/28/2020 20:48:20 - INFO - __main__ -     Total optimization steps = 536\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/268 [00:00<02:53,  1.54it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:01<02:31,  1.76it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:01<02:15,  1.95it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:01<02:05,  2.11it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:02<01:57,  2.24it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:02<01:52,  2.34it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:02<01:48,  2.42it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:03<01:45,  2.46it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:03<01:43,  2.51it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:04<01:41,  2.54it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:04<01:40,  2.56it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:04<01:39,  2.58it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:05<01:38,  2.58it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:05<01:38,  2.59it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:06<01:37,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:06<01:36,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:06<01:37,  2.58it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:07<01:36,  2.59it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:07<01:35,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:07<01:35,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:08<01:34,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:08<01:34,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:09<01:33,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:09<01:34,  2.59it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:09<01:33,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:10<01:33,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:10<01:32,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:11<01:32,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:11<01:31,  2.61it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:11<01:31,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:12<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:12<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:12<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:13<01:30,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:13<01:29,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:14<01:29,  2.60it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:14<01:28,  2.61it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:14<01:28,  2.61it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:15<01:27,  2.61it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:15<01:27,  2.61it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:16<01:27,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:16<01:27,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:16<01:26,  2.60it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:17<01:26,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:17<01:25,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:17<01:25,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:18<01:24,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:18<01:24,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:19<01:23,  2.61it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:19<01:23,  2.60it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:19<01:23,  2.61it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:20<01:22,  2.61it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:20<01:22,  2.61it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:20<01:21,  2.61it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:21<01:21,  2.61it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:21<01:21,  2.61it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:22<01:20,  2.61it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:22<01:20,  2.61it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:22<01:19,  2.61it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:23<01:19,  2.61it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:23<01:19,  2.61it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:24<01:18,  2.61it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:24<01:18,  2.61it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:24<01:18,  2.61it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:25<01:17,  2.61it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:25<01:17,  2.61it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:25<01:17,  2.59it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:26<01:16,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:26<01:16,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:27<01:16,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:27<01:15,  2.60it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:27<01:15,  2.60it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:28<01:14,  2.60it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:28<01:14,  2.61it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:29<01:13,  2.61it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:29<01:13,  2.61it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:29<01:13,  2.61it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:30<01:12,  2.61it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:30<01:12,  2.61it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:30<01:11,  2.61it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:31<01:11,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:31<01:11,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:32<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:32<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:32<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [00:33<01:09,  2.61it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [00:33<01:09,  2.61it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [00:34<01:08,  2.61it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [00:34<01:08,  2.61it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [00:34<01:08,  2.61it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [00:35<01:07,  2.61it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [00:35<01:07,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [00:35<01:06,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [00:36<01:06,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [00:36<01:06,  2.61it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [00:37<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [00:37<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [00:37<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [00:38<01:04,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [00:38<01:04,  2.60it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [00:39<01:04,  2.61it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [00:39<01:03,  2.61it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [00:39<01:03,  2.61it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [00:40<01:02,  2.61it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [00:40<01:02,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [00:40<01:01,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [00:41<01:01,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [00:41<01:01,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [00:42<01:01,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [00:42<01:00,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [00:42<01:00,  2.61it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [00:43<00:59,  2.61it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [00:43<00:59,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [00:43<00:59,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [00:44<00:58,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [00:44<00:58,  2.61it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [00:45<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [00:45<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [00:45<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [00:46<00:56,  2.61it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [00:46<00:56,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [00:47<00:56,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [00:47<00:55,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [00:47<00:55,  2.61it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [00:48<00:54,  2.61it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [00:48<00:54,  2.61it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [00:48<00:54,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [00:49<00:53,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [00:49<00:53,  2.58it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [00:50<00:53,  2.59it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [00:50<00:52,  2.60it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [00:50<00:52,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [00:51<00:51,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [00:51<00:51,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [00:52<00:51,  2.60it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [00:52<00:51,  2.59it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [00:52<00:50,  2.60it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [00:53<00:50,  2.60it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [00:53<00:49,  2.60it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [00:53<00:49,  2.60it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [00:54<00:48,  2.61it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [00:54<00:48,  2.60it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [00:55<00:47,  2.60it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [00:55<00:47,  2.61it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [00:55<00:47,  2.61it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [00:56<00:46,  2.61it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [00:56<00:46,  2.61it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [00:57<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [00:57<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [00:57<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [00:58<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [00:58<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [00:58<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [00:59<00:43,  2.61it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [00:59<00:43,  2.60it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [01:00<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:00<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:00<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:01<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:01<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:02<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:02<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:02<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:03<00:39,  2.61it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:03<00:39,  2.61it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:03<00:39,  2.61it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:04<00:38,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:04<00:38,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:05<00:37,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:05<00:37,  2.61it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:05<00:37,  2.59it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [01:06<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [01:06<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [01:07<00:36,  2.61it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [01:07<00:35,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [01:07<00:35,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [01:08<00:34,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [01:08<00:34,  2.61it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [01:08<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [01:09<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [01:09<00:33,  2.61it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [01:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [01:10<00:32,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [01:10<00:32,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [01:11<00:31,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [01:11<00:31,  2.61it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [01:11<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [01:12<00:30,  2.59it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [01:12<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [01:13<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [01:13<00:29,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [01:13<00:29,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [01:14<00:28,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [01:14<00:28,  2.61it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [01:15<00:27,  2.61it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [01:15<00:27,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [01:15<00:27,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [01:16<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [01:16<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [01:16<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [01:17<00:25,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [01:17<00:25,  2.59it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [01:18<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [01:18<00:24,  2.59it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [01:18<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [01:19<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [01:19<00:23,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [01:20<00:23,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [01:20<00:22,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [01:20<00:22,  2.61it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [01:21<00:21,  2.61it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [01:21<00:21,  2.61it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [01:21<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [01:22<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [01:22<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [01:23<00:19,  2.61it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [01:23<00:19,  2.61it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [01:23<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [01:24<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [01:24<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [01:25<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [01:25<00:17,  2.61it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [01:25<00:17,  2.61it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [01:26<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [01:26<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [01:26<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [01:27<00:15,  2.61it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [01:27<00:15,  2.61it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [01:28<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [01:28<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [01:28<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  87% 232/268 [01:29<00:13,  2.61it/s]\u001b[A\n",
            "Iteration:  87% 233/268 [01:29<00:13,  2.61it/s]\u001b[A\n",
            "Iteration:  87% 234/268 [01:30<00:13,  2.61it/s]\u001b[A\n",
            "Iteration:  88% 235/268 [01:30<00:12,  2.61it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [01:30<00:12,  2.61it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [01:31<00:11,  2.61it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [01:31<00:11,  2.61it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [01:31<00:11,  2.61it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [01:32<00:10,  2.61it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [01:32<00:10,  2.61it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [01:33<00:09,  2.61it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [01:33<00:09,  2.61it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [01:33<00:09,  2.61it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [01:34<00:08,  2.61it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [01:34<00:08,  2.61it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [01:35<00:08,  2.61it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [01:35<00:07,  2.61it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [01:35<00:07,  2.59it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [01:36<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [01:36<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [01:36<00:06,  2.61it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [01:37<00:05,  2.61it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [01:37<00:05,  2.61it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [01:38<00:04,  2.61it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [01:38<00:04,  2.61it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [01:38<00:04,  2.61it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [01:39<00:03,  2.61it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [01:39<00:03,  2.61it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [01:39<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [01:40<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [01:40<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [01:41<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [01:41<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [01:41<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [01:42<00:00,  2.61it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [01:42<00:00,  2.61it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [01:42<00:00,  2.61it/s]\n",
            "Epoch:  50% 1/2 [01:42<01:42, 102.80s/it]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/268 [00:00<01:42,  2.61it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:00<01:42,  2.61it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:01<01:41,  2.61it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:01<01:41,  2.61it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:01<01:40,  2.61it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:02<01:40,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:02<01:39,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:03<01:39,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:03<01:39,  2.61it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:03<01:38,  2.61it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:04<01:38,  2.61it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:04<01:37,  2.62it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:04<01:38,  2.59it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:05<01:37,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:05<01:37,  2.61it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:06<01:36,  2.61it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:06<01:36,  2.61it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:06<01:35,  2.61it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:07<01:35,  2.61it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:07<01:35,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:08<01:34,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:08<01:34,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:08<01:33,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:09<01:33,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:09<01:33,  2.61it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:09<01:32,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:10<01:32,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:10<01:32,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:11<01:31,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:11<01:31,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:11<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:12<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:12<01:30,  2.61it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:13<01:29,  2.61it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:13<01:29,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:13<01:29,  2.61it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:14<01:28,  2.60it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:14<01:28,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:14<01:28,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:15<01:27,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:15<01:27,  2.60it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:16<01:26,  2.61it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:16<01:26,  2.61it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:16<01:25,  2.61it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:17<01:25,  2.61it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:17<01:25,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:18<01:24,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:18<01:24,  2.61it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:18<01:24,  2.60it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:19<01:23,  2.61it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:19<01:23,  2.61it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:19<01:23,  2.59it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:20<01:22,  2.59it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:20<01:22,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:21<01:22,  2.59it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:21<01:21,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:21<01:21,  2.60it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:22<01:20,  2.61it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:22<01:20,  2.61it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:23<01:19,  2.61it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:23<01:19,  2.61it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:23<01:18,  2.61it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:24<01:18,  2.60it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:24<01:18,  2.61it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:24<01:17,  2.61it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:25<01:17,  2.61it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:25<01:17,  2.61it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:26<01:16,  2.61it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:26<01:16,  2.61it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:26<01:15,  2.61it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:27<01:15,  2.61it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:27<01:15,  2.61it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:28<01:14,  2.61it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:28<01:14,  2.61it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:28<01:14,  2.61it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:29<01:13,  2.61it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:29<01:13,  2.61it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:29<01:12,  2.60it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:30<01:12,  2.61it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:30<01:12,  2.61it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:31<01:11,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:31<01:11,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:31<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:32<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:32<01:10,  2.61it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [00:32<01:10,  2.59it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [00:33<01:09,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [00:33<01:09,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [00:34<01:08,  2.61it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [00:34<01:08,  2.61it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [00:34<01:08,  2.60it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [00:35<01:07,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [00:35<01:07,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [00:36<01:06,  2.61it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [00:36<01:06,  2.61it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [00:36<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [00:37<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [00:37<01:05,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [00:37<01:04,  2.61it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [00:38<01:04,  2.61it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [00:38<01:03,  2.61it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [00:39<01:03,  2.61it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [00:39<01:03,  2.61it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [00:39<01:02,  2.61it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [00:40<01:02,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [00:40<01:02,  2.61it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [00:41<01:01,  2.60it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [00:41<01:01,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [00:41<01:00,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [00:42<01:00,  2.61it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [00:42<01:00,  2.61it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [00:42<01:00,  2.60it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [00:43<00:59,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [00:43<00:59,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [00:44<00:58,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [00:44<00:58,  2.60it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [00:44<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [00:45<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [00:45<00:57,  2.61it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [00:46<00:56,  2.60it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [00:46<00:56,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [00:46<00:56,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [00:47<00:55,  2.61it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [00:47<00:55,  2.60it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [00:47<00:54,  2.61it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [00:48<00:54,  2.60it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [00:48<00:54,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [00:49<00:53,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [00:49<00:53,  2.61it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [00:49<00:52,  2.61it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [00:50<00:52,  2.61it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [00:50<00:52,  2.61it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [00:51<00:51,  2.61it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [00:51<00:51,  2.61it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [00:51<00:50,  2.61it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [00:52<00:50,  2.61it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [00:52<00:50,  2.61it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [00:52<00:50,  2.60it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [00:53<00:49,  2.61it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [00:53<00:49,  2.61it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [00:54<00:48,  2.61it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [00:54<00:48,  2.61it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [00:54<00:47,  2.61it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [00:55<00:47,  2.61it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [00:55<00:47,  2.61it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [00:55<00:46,  2.61it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [00:56<00:46,  2.61it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [00:56<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [00:57<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [00:57<00:45,  2.61it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [00:57<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [00:58<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [00:58<00:44,  2.61it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [00:59<00:43,  2.61it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [00:59<00:43,  2.61it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [00:59<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:00<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:00<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:00<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:01<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:01<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:02<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:02<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:02<00:39,  2.61it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:03<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:03<00:39,  2.61it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:04<00:38,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:04<00:38,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:04<00:37,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:05<00:37,  2.61it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:05<00:37,  2.61it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [01:05<00:36,  2.61it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [01:06<00:36,  2.61it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [01:06<00:35,  2.61it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [01:07<00:35,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [01:07<00:35,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [01:07<00:34,  2.61it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [01:08<00:34,  2.61it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [01:08<00:34,  2.61it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [01:09<00:33,  2.61it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [01:09<00:33,  2.61it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [01:09<00:32,  2.61it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [01:10<00:32,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [01:10<00:32,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [01:10<00:31,  2.61it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [01:11<00:31,  2.61it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [01:11<00:30,  2.61it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [01:12<00:30,  2.61it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [01:12<00:30,  2.61it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [01:12<00:29,  2.61it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [01:13<00:29,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [01:13<00:29,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [01:14<00:28,  2.61it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [01:14<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [01:14<00:28,  2.61it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [01:15<00:27,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [01:15<00:27,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [01:15<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [01:16<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [01:16<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [01:17<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [01:17<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [01:17<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [01:18<00:24,  2.61it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [01:18<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [01:18<00:23,  2.61it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [01:19<00:23,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [01:19<00:22,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [01:20<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [01:20<00:22,  2.61it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [01:20<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [01:21<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [01:21<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [01:22<00:20,  2.59it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [01:22<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [01:22<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [01:23<00:19,  2.61it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [01:23<00:19,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [01:23<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [01:24<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [01:24<00:18,  2.61it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [01:25<00:17,  2.61it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [01:25<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [01:25<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [01:26<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [01:26<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [01:27<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [01:27<00:15,  2.61it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [01:27<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [01:28<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [01:28<00:14,  2.61it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 1.3432835820895524e-06, \"loss\": 0.4096446013972163, \"step\": 500}\n",
            "03/28/2020 20:51:31 - INFO - transformers.configuration_utils -   Configuration saved in cola_output/checkpoint-500/config.json\n",
            "03/28/2020 20:51:32 - INFO - transformers.modeling_utils -   Model weights saved in cola_output/checkpoint-500/pytorch_model.bin\n",
            "03/28/2020 20:51:33 - INFO - __main__ -   Saving model checkpoint to cola_output/checkpoint-500\n",
            "03/28/2020 20:51:36 - INFO - __main__ -   Saving optimizer and scheduler states to cola_output/checkpoint-500\n",
            "\n",
            "Iteration:  87% 232/268 [01:33<01:01,  1.71s/it]\u001b[A\n",
            "Iteration:  87% 233/268 [01:33<00:46,  1.32s/it]\u001b[A\n",
            "Iteration:  87% 234/268 [01:34<00:35,  1.04s/it]\u001b[A\n",
            "Iteration:  88% 235/268 [01:34<00:27,  1.19it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [01:34<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [01:35<00:18,  1.64it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [01:35<00:16,  1.85it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [01:36<00:14,  2.03it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [01:36<00:12,  2.17it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [01:36<00:11,  2.29it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [01:37<00:10,  2.38it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [01:37<00:10,  2.44it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [01:38<00:09,  2.49it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [01:38<00:09,  2.52it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [01:38<00:08,  2.55it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [01:39<00:08,  2.57it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [01:39<00:07,  2.58it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [01:39<00:07,  2.59it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [01:40<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [01:40<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [01:41<00:06,  2.61it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [01:41<00:05,  2.61it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [01:41<00:05,  2.61it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [01:42<00:04,  2.61it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [01:42<00:04,  2.61it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [01:43<00:04,  2.59it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [01:43<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [01:43<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [01:44<00:03,  2.59it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [01:44<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [01:44<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [01:45<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [01:45<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [01:46<00:01,  2.61it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [01:46<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [01:46<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [01:46<00:00,  2.51it/s]\n",
            "Epoch: 100% 2/2 [03:29<00:00, 104.89s/it]\n",
            "03/28/2020 20:51:49 - INFO - __main__ -    global_step = 536, average loss = 0.40363558381795883\n",
            "03/28/2020 20:51:49 - INFO - __main__ -   Saving model checkpoint to cola_output\n",
            "03/28/2020 20:51:49 - INFO - transformers.configuration_utils -   Configuration saved in cola_output/config.json\n",
            "03/28/2020 20:51:50 - INFO - transformers.modeling_utils -   Model weights saved in cola_output/pytorch_model.bin\n",
            "03/28/2020 20:51:50 - INFO - transformers.configuration_utils -   loading configuration file cola_output/config.json\n",
            "03/28/2020 20:51:50 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:51:50 - INFO - transformers.modeling_utils -   loading weights file cola_output/pytorch_model.bin\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   Model name 'cola_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'cola_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   Didn't find file cola_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/vocab.txt\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/special_tokens_map.json\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/tokenizer_config.json\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   Model name 'cola_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'cola_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   Didn't find file cola_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/vocab.txt\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/special_tokens_map.json\n",
            "03/28/2020 20:51:53 - INFO - transformers.tokenization_utils -   loading file cola_output/tokenizer_config.json\n",
            "03/28/2020 20:51:53 - INFO - __main__ -   Evaluate the following checkpoints: ['cola_output']\n",
            "03/28/2020 20:51:53 - INFO - transformers.configuration_utils -   loading configuration file cola_output/config.json\n",
            "03/28/2020 20:51:53 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:51:53 - INFO - transformers.modeling_utils -   loading weights file cola_output/pytorch_model.bin\n",
            "03/28/2020 20:51:56 - INFO - __main__ -   Creating features from dataset file at glue_data//CoLA/\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:51:56 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:51:56 - INFO - __main__ -   Saving features into cached file glue_data//CoLA/cached_dev_bert-base-uncased_128_cola\n",
            "03/28/2020 20:51:56 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/28/2020 20:51:56 - INFO - __main__ -     Num examples = 1043\n",
            "03/28/2020 20:51:56 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 131/131 [00:04<00:00, 29.75it/s]\n",
            "03/28/2020 20:52:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/28/2020 20:52:00 - INFO - __main__ -     mcc = 0.5675730029368427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84LIYDiccxcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07c41d33-8ecf-4e22-daaa-2c0cf4dfde58"
      },
      "source": [
        "GLUE_DIR=\"glue_data/\"\n",
        "\n",
        "!python run_glue.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path 'cola_output' \\\n",
        "  --task_name MRPC \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir $GLUE_DIR/MRPC/ \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_gpu_train_batch_size 32 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --output_dir 'mrpc_output' \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-28 20:53:31.768194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "03/28/2020 20:53:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/28/2020 20:53:33 - INFO - transformers.configuration_utils -   loading configuration file cola_output/config.json\n",
            "03/28/2020 20:53:33 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   Model name 'cola_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'cola_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   Didn't find file cola_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   loading file cola_output/vocab.txt\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   loading file cola_output/special_tokens_map.json\n",
            "03/28/2020 20:53:33 - INFO - transformers.tokenization_utils -   loading file cola_output/tokenizer_config.json\n",
            "03/28/2020 20:53:33 - INFO - transformers.modeling_utils -   loading weights file cola_output/pytorch_model.bin\n",
            "03/28/2020 20:53:39 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data//MRPC/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='cola_output', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='mrpc_output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=500, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/28/2020 20:53:39 - INFO - __main__ -   Creating features from dataset file at glue_data//MRPC/\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   LOOKING AT glue_data//MRPC/train.tsv\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   Writing example 0/3668\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:53:39 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:53:42 - INFO - __main__ -   Saving features into cached file glue_data//MRPC/cached_train_cola_output_128_mrpc\n",
            "03/28/2020 20:53:43 - INFO - __main__ -   ***** Running training *****\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Num examples = 3668\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Num Epochs = 5\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/28/2020 20:53:43 - INFO - __main__ -     Total optimization steps = 575\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<00:46,  2.45it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:00<00:45,  2.49it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:01<00:44,  2.53it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:01<00:43,  2.55it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:01<00:42,  2.56it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:02<00:42,  2.57it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:02<00:41,  2.58it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:03<00:41,  2.59it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:03<00:40,  2.59it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:03<00:40,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:04<00:40,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:04<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:05<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:05<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:05<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:06<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:07<00:36,  2.59it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:08<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:09<00:35,  2.59it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:09<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:10<00:34,  2.59it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:10<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:11<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:11<00:32,  2.59it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:11<00:32,  2.59it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:13<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:15<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:15<00:28,  2.61it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:15<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:16<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:18<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:18<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:18<00:25,  2.61it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:19<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:19<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:20<00:24,  2.58it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:20<00:23,  2.58it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:20<00:23,  2.59it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:21<00:23,  2.57it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:21<00:22,  2.58it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:21<00:22,  2.59it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:22<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:22<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:23<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:23<00:20,  2.59it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:23<00:20,  2.58it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:24<00:20,  2.58it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:24<00:19,  2.58it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:25<00:19,  2.59it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:25<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:25<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:26<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:26<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:26<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:27<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:27<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:28<00:16,  2.61it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:28<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:28<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:29<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:29<00:14,  2.61it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:30<00:14,  2.59it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:30<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:30<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:31<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:31<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [00:31<00:12,  2.59it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [00:32<00:11,  2.59it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [00:33<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [00:34<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [00:34<00:09,  2.59it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [00:35<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [00:36<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [00:36<00:07,  2.59it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [00:37<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [00:37<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [00:38<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [00:38<00:05,  2.57it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [00:39<00:05,  2.58it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [00:39<00:04,  2.59it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [00:40<00:04,  2.58it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [00:40<00:03,  2.58it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [00:40<00:03,  2.57it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [00:41<00:03,  2.58it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [00:41<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [00:42<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [00:42<00:01,  2.59it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [00:42<00:01,  2.59it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [00:43<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [00:44<00:00,  2.60it/s]\n",
            "Epoch:  20% 1/5 [00:44<02:56, 44.23s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<00:43,  2.61it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:00<00:43,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:01<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:01<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:01<00:42,  2.59it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:02<00:42,  2.59it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:02<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:03<00:41,  2.59it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:03<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:03<00:40,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:04<00:40,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:04<00:40,  2.57it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:05<00:39,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:05<00:39,  2.58it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:05<00:38,  2.58it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:06<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:06<00:37,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:06<00:37,  2.59it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:08<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:09<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:09<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:10<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:10<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:11<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:11<00:32,  2.59it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:11<00:32,  2.59it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:12<00:31,  2.59it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:13<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:13<00:30,  2.59it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:14<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:14<00:29,  2.58it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:15<00:29,  2.59it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:15<00:28,  2.59it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:15<00:28,  2.59it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:16<00:28,  2.59it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:18<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:18<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:18<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:19<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:19<00:24,  2.58it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:20<00:24,  2.59it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:21<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:22<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:22<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:23<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:23<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:23<00:20,  2.59it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:24<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:24<00:19,  2.57it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:25<00:19,  2.57it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:25<00:18,  2.58it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:25<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:26<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:26<00:17,  2.59it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:26<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:27<00:16,  2.59it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:27<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:28<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:28<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:28<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:29<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:29<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:30<00:14,  2.58it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:30<00:13,  2.59it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:30<00:13,  2.59it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:31<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:31<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [00:31<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [00:33<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [00:33<00:10,  2.59it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [00:34<00:10,  2.59it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [00:34<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [00:35<00:09,  2.59it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [00:36<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [00:37<00:07,  2.58it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [00:37<00:06,  2.59it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [00:37<00:06,  2.59it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [00:38<00:06,  2.59it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [00:39<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [00:39<00:04,  2.59it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [00:40<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [00:40<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [00:40<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [00:41<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [00:41<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [00:42<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [00:43<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [00:44<00:00,  2.60it/s]\n",
            "Epoch:  40% 2/5 [01:28<02:12, 44.23s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<00:43,  2.62it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:00<00:43,  2.62it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:01<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:01<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:01<00:42,  2.61it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:02<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:02<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:03<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:03<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:03<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:04<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:04<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:04<00:39,  2.59it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:05<00:39,  2.59it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:05<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:06<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:06<00:38,  2.57it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:06<00:37,  2.58it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:07<00:37,  2.58it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:07<00:36,  2.59it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:08<00:36,  2.59it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:08<00:35,  2.59it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:09<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:09<00:34,  2.59it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:10<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:10<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:11<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:11<00:32,  2.60it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:11<00:32,  2.60it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:13<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:15<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:15<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:15<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:16<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:17<00:26,  2.60it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:18<00:26,  2.61it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:18<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:18<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:19<00:25,  2.60it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:19<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:20<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:21<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:22<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:22<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:23<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:23<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:23<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:24<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:24<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:25<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:25<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:25<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:26<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:26<00:17,  2.58it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:26<00:17,  2.59it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:27<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:27<00:16,  2.59it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:28<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:28<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:28<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:29<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:29<00:14,  2.59it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:30<00:14,  2.59it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:30<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:30<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:31<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:31<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [00:31<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [00:33<00:11,  2.61it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [00:34<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [00:34<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [00:35<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [00:35<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [00:36<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [00:37<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [00:37<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [00:38<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [00:39<00:05,  2.59it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [00:39<00:04,  2.59it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [00:40<00:04,  2.59it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [00:40<00:03,  2.59it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [00:40<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [00:41<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [00:41<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [00:41<00:02,  2.60it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [00:43<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [00:43<00:00,  2.59it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [00:44<00:00,  2.60it/s]\n",
            "Epoch:  60% 3/5 [02:12<01:28, 44.21s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<00:44,  2.58it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:00<00:43,  2.59it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:01<00:43,  2.59it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:01<00:42,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:01<00:42,  2.59it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:02<00:42,  2.59it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:02<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:03<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:03<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:03<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:04<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:04<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:05<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:05<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:05<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:06<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:08<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:08<00:35,  2.59it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:09<00:35,  2.58it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:09<00:34,  2.58it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:10<00:34,  2.58it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:10<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:10<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:11<00:33,  2.59it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:11<00:32,  2.60it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:11<00:32,  2.60it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:13<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:13<00:30,  2.59it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:14<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:15<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:15<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:15<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:16<00:28,  2.60it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:16<00:27,  2.60it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:16<00:27,  2.58it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:17<00:27,  2.59it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:17<00:26,  2.59it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:18<00:26,  2.59it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:18<00:25,  2.59it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:18<00:25,  2.59it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:19<00:25,  2.59it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:19<00:24,  2.59it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:20<00:24,  2.60it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:20<00:23,  2.60it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:21<00:23,  2.59it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:21<00:22,  2.60it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:22<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:22<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:23<00:21,  2.60it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:23<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:23<00:20,  2.59it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:24<00:20,  2.59it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:24<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:25<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:25<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:25<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:26<00:18,  2.58it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:26<00:17,  2.59it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:26<00:17,  2.59it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:27<00:16,  2.59it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:27<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:28<00:16,  2.58it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:28<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:28<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:29<00:15,  2.59it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:29<00:14,  2.59it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:30<00:14,  2.59it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:30<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:30<00:13,  2.59it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:31<00:13,  2.59it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:31<00:12,  2.59it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [00:31<00:12,  2.59it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [00:32<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [00:33<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [00:33<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [00:34<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [00:34<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [00:35<00:09,  2.61it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [00:35<00:08,  2.61it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [00:35<00:08,  2.61it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [00:36<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [00:36<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [00:37<00:06,  2.59it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [00:37<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [00:38<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [00:38<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [00:39<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [00:39<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [00:40<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [00:40<00:03,  2.61it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [00:40<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [00:41<00:03,  2.58it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [00:41<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [00:41<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [00:42<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [00:43<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [00:43<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [00:44<00:00,  2.60it/s]\n",
            "Epoch:  80% 4/5 [02:56<00:44, 44.20s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<00:43,  2.59it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:00<00:43,  2.60it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:01<00:43,  2.60it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:01<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:01<00:42,  2.60it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:02<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:02<00:41,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:03<00:41,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:03<00:40,  2.61it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:03<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:04<00:40,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:04<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:04<00:39,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:05<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:05<00:38,  2.59it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:06<00:38,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:06<00:37,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:07<00:36,  2.60it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:08<00:36,  2.58it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:08<00:35,  2.59it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:08<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:09<00:35,  2.60it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:09<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:10<00:34,  2.60it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:10<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:11<00:33,  2.60it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:11<00:32,  2.60it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:11<00:32,  2.59it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:12<00:31,  2.60it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:12<00:31,  2.58it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:13<00:31,  2.59it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:13<00:30,  2.59it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:13<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:14<00:30,  2.60it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:14<00:29,  2.60it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:15<00:29,  2.60it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 2.6086956521739132e-06, \"loss\": 0.32445381727814676, \"step\": 500}\n",
            "03/28/2020 20:56:55 - INFO - transformers.configuration_utils -   Configuration saved in mrpc_output/checkpoint-500/config.json\n",
            "03/28/2020 20:56:56 - INFO - transformers.modeling_utils -   Model weights saved in mrpc_output/checkpoint-500/pytorch_model.bin\n",
            "03/28/2020 20:56:56 - INFO - __main__ -   Saving model checkpoint to mrpc_output/checkpoint-500\n",
            "03/28/2020 20:56:59 - INFO - __main__ -   Saving optimizer and scheduler states to mrpc_output/checkpoint-500\n",
            "\n",
            "Iteration:  35% 40/115 [00:19<02:09,  1.73s/it]\u001b[A\n",
            "Iteration:  36% 41/115 [00:20<01:38,  1.33s/it]\u001b[A\n",
            "Iteration:  37% 42/115 [00:20<01:16,  1.05s/it]\u001b[A\n",
            "Iteration:  37% 43/115 [00:21<01:01,  1.18it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:21<00:50,  1.41it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:21<00:42,  1.63it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:22<00:37,  1.83it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:22<00:33,  2.01it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:22<00:31,  2.16it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:23<00:28,  2.28it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:23<00:27,  2.37it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:24<00:26,  2.43it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:24<00:25,  2.48it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:24<00:24,  2.51it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:25<00:24,  2.53it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:25<00:23,  2.56it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:26<00:22,  2.57it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:26<00:22,  2.58it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:26<00:22,  2.59it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:27<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:27<00:21,  2.59it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:27<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:28<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:28<00:20,  2.60it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:29<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:29<00:19,  2.60it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:29<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:30<00:18,  2.59it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:30<00:18,  2.60it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:31<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:31<00:17,  2.60it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:31<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:32<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:32<00:16,  2.60it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:32<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:33<00:15,  2.60it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:33<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:34<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:34<00:14,  2.60it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:34<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:35<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:35<00:13,  2.60it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:36<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [00:36<00:12,  2.60it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [00:36<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [00:37<00:11,  2.61it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [00:37<00:11,  2.60it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [00:37<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [00:38<00:10,  2.60it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [00:38<00:09,  2.61it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [00:39<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [00:39<00:09,  2.60it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [00:39<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [00:40<00:08,  2.59it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [00:40<00:08,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [00:41<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [00:41<00:07,  2.60it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [00:41<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [00:42<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [00:42<00:06,  2.60it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [00:42<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [00:43<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [00:43<00:05,  2.60it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [00:44<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [00:44<00:04,  2.60it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [00:44<00:03,  2.59it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [00:45<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [00:45<00:03,  2.60it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [00:46<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [00:46<00:02,  2.59it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [00:46<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [00:47<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [00:47<00:01,  2.60it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [00:47<00:00,  2.60it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [00:48<00:00,  2.60it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [00:48<00:00,  2.36it/s]\n",
            "Epoch: 100% 5/5 [03:45<00:00, 45.09s/it]\n",
            "03/28/2020 20:57:28 - INFO - __main__ -    global_step = 575, average loss = 0.29440549055195375\n",
            "03/28/2020 20:57:28 - INFO - __main__ -   Saving model checkpoint to mrpc_output\n",
            "03/28/2020 20:57:28 - INFO - transformers.configuration_utils -   Configuration saved in mrpc_output/config.json\n",
            "03/28/2020 20:57:29 - INFO - transformers.modeling_utils -   Model weights saved in mrpc_output/pytorch_model.bin\n",
            "03/28/2020 20:57:29 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/28/2020 20:57:29 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:57:29 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/28/2020 20:57:32 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/28/2020 20:57:32 - INFO - __main__ -   Evaluate the following checkpoints: ['mrpc_output']\n",
            "03/28/2020 20:57:32 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/28/2020 20:57:32 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 20:57:32 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/28/2020 20:57:34 - INFO - __main__ -   Creating features from dataset file at glue_data//MRPC/\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 20:57:34 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 20:57:35 - INFO - __main__ -   Saving features into cached file glue_data//MRPC/cached_dev_cola_output_128_mrpc\n",
            "03/28/2020 20:57:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/28/2020 20:57:35 - INFO - __main__ -     Num examples = 408\n",
            "03/28/2020 20:57:35 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 51/51 [00:01<00:00, 29.64it/s]\n",
            "03/28/2020 20:57:36 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/28/2020 20:57:36 - INFO - __main__ -     acc = 0.8651960784313726\n",
            "03/28/2020 20:57:36 - INFO - __main__ -     acc_and_f1 = 0.8862236715934266\n",
            "03/28/2020 20:57:36 - INFO - __main__ -     f1 = 0.9072512647554806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJUwgEmO5nj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "327b102b-a51f-4726-c4e5-1d2c663b5448"
      },
      "source": [
        "!python run_glue.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path 'mrpc_output' \\\n",
        "  --task_name CoLA \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --data_dir $GLUE_DIR/CoLA \\\n",
        "  --max_seq_length 128 \\\n",
        "  --output_dir 'mrpc_output' \\"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-28 21:11:31.804774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "03/28/2020 21:11:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/28/2020 21:11:33 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/28/2020 21:11:33 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/28/2020 21:11:33 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/28/2020 21:11:33 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/28/2020 21:11:39 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='glue_data//CoLA', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='mrpc_output', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='mrpc_output', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   Model name 'mrpc_output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'mrpc_output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   Didn't find file mrpc_output/added_tokens.json. We won't load it.\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   loading file mrpc_output/vocab.txt\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   loading file mrpc_output/special_tokens_map.json\n",
            "03/28/2020 21:11:39 - INFO - transformers.tokenization_utils -   loading file mrpc_output/tokenizer_config.json\n",
            "03/28/2020 21:11:39 - INFO - __main__ -   Evaluate the following checkpoints: ['mrpc_output']\n",
            "03/28/2020 21:11:39 - INFO - transformers.configuration_utils -   loading configuration file mrpc_output/config.json\n",
            "03/28/2020 21:11:39 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/28/2020 21:11:39 - INFO - transformers.modeling_utils -   loading weights file mrpc_output/pytorch_model.bin\n",
            "03/28/2020 21:11:42 - INFO - __main__ -   Creating features from dataset file at glue_data//CoLA\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/28/2020 21:11:42 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "03/28/2020 21:11:42 - INFO - __main__ -   Saving features into cached file glue_data//CoLA/cached_dev_mrpc_output_128_cola\n",
            "03/28/2020 21:11:42 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/28/2020 21:11:42 - INFO - __main__ -     Num examples = 1043\n",
            "03/28/2020 21:11:42 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 131/131 [00:04<00:00, 29.44it/s]\n",
            "03/28/2020 21:11:47 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/28/2020 21:11:47 - INFO - __main__ -     mcc = 0.3359888261102686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIZiYcV18xWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}